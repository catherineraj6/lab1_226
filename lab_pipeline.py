# -*- coding: utf-8 -*-
"""lab_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MQ7nGzr9GZ4Tg8C9sj1jmQO80AWbmXGX
"""

# In Cloud Composer, add snowflake-connector-python to PYPI Packages
from airflow import DAG
from airflow.models import Variable
from airflow.decorators import task

from datetime import timedelta, datetime
import snowflake.connector
import requests


# Snowflake connection function
def return_snowflake_conn():

    user_id = Variable.get('snowflake_userid')
    password = Variable.get('snowflake_password')
    account = Variable.get('snowflake_account')

    # Establish a connection to Snowflake
    conn = snowflake.connector.connect(
        user=user_id,
        password=password,
        account=account,  # Example: 'xyz12345.us-east-1'
        warehouse='compute_wh',
        database='dev2'  # Updated to the 'dev2' database as per your new schema
    )
    # Create a cursor object
    return conn.cursor()


@task
def extract(symbol):
    """Extracts the last 90 days of stock data from Alpha Vantage API for a given symbol."""
    api_key = Variable.get('vantage_api_key')
    url = f"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&apikey={api_key}"
    response = requests.get(url)
    return response.json()


@task
def transform(data):
    """Transforms the raw API data into a structured format."""
    time_series = data.get('Time Series (Daily)', {})
    symbol = data.get('Meta Data', {}).get('2. Symbol', '')

    records = []
    for date, values in time_series.items():
        transformed = {
            'symbol': symbol,
            'date': date,
            'open': values['1. open'],
            'close': values['4. close'],
            'min': values['3. low'],
            'max': values['2. high'],
            'volume': values['5. volume']
        }
        records.append(transformed)

    # Return only the last 90 days of data
    return records[:90]  # Assuming data is in descending order by date


@task
def load(cur, records, target_table):
    """Loads the transformed stock data into Snowflake."""
    try:
        cur.execute("BEGIN;")
        cur.execute(f"""
        CREATE OR REPLACE TABLE {target_table} (
            symbol VARCHAR,
            date DATE,
            open FLOAT,
            close FLOAT,
            min FLOAT,
            max FLOAT,
            volume BIGINT
        )
        """)

        for record in records:
            sql = f"""
            INSERT INTO {target_table} (symbol, date, open, close, min, max, volume)
            VALUES ('{record['symbol']}', '{record['date']}', {record['open']}, {record['close']},
                    {record['min']}, {record['max']}, {record['volume']})
            """
            cur.execute(sql)
        cur.execute("COMMIT;")
    except Exception as e:
        cur.execute("ROLLBACK;")
        print(e)
        raise e


with DAG(
    dag_id='StockDataPipeline2',
    start_date=datetime(2024, 9, 21),
    catchup=False,
    tags=['ETL', 'stock'],
    schedule_interval='0 2 * * *'
) as dag:

    target_table = "dev2.rawdata2.stock_prices"  # Updated schema and database as per your new details
    cur = return_snowflake_conn()

    # Extract data for GOOG and AAPL
    goog_data = extract('GOOG')
    aapl_data = extract('AAPL')

    # Transform the data
    transformed_goog = transform(goog_data)
    transformed_aapl = transform(aapl_data)

    # Load the data into Snowflake
    load(cur, transformed_goog, target_table)
    load(cur, transformed_aapl, target_table)